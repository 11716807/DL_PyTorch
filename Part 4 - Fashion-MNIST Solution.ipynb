{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Exercise\n",
    "\n",
    "Now it's your turn to build a neural network. You'll be using the [Fashion-MNIST dataset](https://github.com/zalandoresearch/fashion-mnist), a drop-in replacement for the MNIST dataset. MNIST is actually quite trivial with neural networks where you can easily achieve better than 97% accuracy. Fashion-MNIST is a set of 28x28 greyscale images of clothes. It's more complex than MNIST, so it's a better representation of the actual performance of your network.\n",
    "\n",
    "<img src='assets/fashion-mnist-sprite.png' width=500px>\n",
    "\n",
    "In this notebook, you'll build your own neural network. For the most part, you could just copy and paste the code from Part 3, but you wouldn't be learning. It's important for you to write the code yourself and get it to work. Feel free to consult the previous notebook though as you work through this.\n",
    "\n",
    "First off, let's load the dataset through torchvision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import helper\n",
    "\n",
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "# Download and load the training data\n",
    "trainset = datasets.FashionMNIST('F_MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Download and load the test data\n",
    "testset = datasets.FashionMNIST('F_MNIST_data/', download=True, train=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see one of the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x105e0d278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image, label = next(iter(trainloader))\n",
    "helper.imshow(image[0,:]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the data loaded, it's time to import the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import helper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the network\n",
    "\n",
    "Here you should define your network. As with MNIST, each image is 28x28 which is a total of 784 pixels, and there are 10 classes. You should include at least one hidden layer. We suggest you use ReLU activations for the layers and softmax for the output to get the class probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        # One hidden layer\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ''' Forward pass through the network, returns the output logits '''\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the network\n",
    "\n",
    "Now you should create your network and train it. First you'll want to define [the criterion](http://pytorch.org/docs/master/nn.html#loss-functions) ( something like `nn.CrossEntropyLoss`) and [the optimizer](http://pytorch.org/docs/master/optim.html) (typically `optim.SGD` or `optim.Adam`).\n",
    "\n",
    "Then write the training code. Remember the training pass is a fairly straightforward process:\n",
    "\n",
    "* Make a forward pass through the network to get the logits \n",
    "* Use the logits to calculate the loss\n",
    "* Perform a backward pass through the network with `loss.backward()` to calculate the gradients\n",
    "* Take a step with the optimizer to update the weights\n",
    "\n",
    "By adjusting the hyperparameters (hidden units, learning rate, etc), you should be able to get 85-86% validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the network, define the criterion and optimizer\n",
    "net = Network()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/2 Loss: 1.8993 Test accuracy: 0.6181\n",
      "Epoch: 1/2 Loss: 1.2003 Test accuracy: 0.6908\n",
      "Epoch: 1/2 Loss: 0.8520 Test accuracy: 0.6994\n",
      "Epoch: 1/2 Loss: 0.7673 Test accuracy: 0.7331\n",
      "Epoch: 1/2 Loss: 0.6902 Test accuracy: 0.7418\n",
      "Epoch: 1/2 Loss: 0.6623 Test accuracy: 0.7380\n",
      "Epoch: 1/2 Loss: 0.6534 Test accuracy: 0.7538\n",
      "Epoch: 1/2 Loss: 0.6260 Test accuracy: 0.7607\n",
      "Epoch: 1/2 Loss: 0.6447 Test accuracy: 0.7651\n",
      "Epoch: 1/2 Loss: 0.5183 Test accuracy: 0.7761\n",
      "Epoch: 1/2 Loss: 0.5235 Test accuracy: 0.7690\n",
      "Epoch: 1/2 Loss: 0.5884 Test accuracy: 0.7858\n",
      "Epoch: 1/2 Loss: 0.6022 Test accuracy: 0.7708\n",
      "Epoch: 1/2 Loss: 0.5312 Test accuracy: 0.7895\n",
      "Epoch: 1/2 Loss: 0.5831 Test accuracy: 0.7781\n",
      "Epoch: 1/2 Loss: 0.6046 Test accuracy: 0.7763\n",
      "Epoch: 1/2 Loss: 0.5941 Test accuracy: 0.7882\n",
      "Epoch: 1/2 Loss: 0.5672 Test accuracy: 0.7894\n",
      "Epoch: 1/2 Loss: 0.5684 Test accuracy: 0.7965\n",
      "Epoch: 1/2 Loss: 0.5890 Test accuracy: 0.8064\n",
      "Epoch: 1/2 Loss: 0.5232 Test accuracy: 0.8147\n",
      "Epoch: 1/2 Loss: 0.4863 Test accuracy: 0.8069\n",
      "Epoch: 1/2 Loss: 0.5115 Test accuracy: 0.8042\n",
      "Epoch: 1/2 Loss: 0.5616 Test accuracy: 0.7922\n",
      "Epoch: 1/2 Loss: 0.5375 Test accuracy: 0.8046\n",
      "Epoch: 1/2 Loss: 0.5337 Test accuracy: 0.8059\n",
      "Epoch: 1/2 Loss: 0.5469 Test accuracy: 0.8173\n",
      "Epoch: 1/2 Loss: 0.4265 Test accuracy: 0.8199\n",
      "Epoch: 1/2 Loss: 0.4802 Test accuracy: 0.8188\n",
      "Epoch: 1/2 Loss: 0.4765 Test accuracy: 0.8232\n",
      "Epoch: 1/2 Loss: 0.5073 Test accuracy: 0.8135\n",
      "Epoch: 1/2 Loss: 0.4557 Test accuracy: 0.8170\n",
      "Epoch: 1/2 Loss: 0.5477 Test accuracy: 0.8131\n",
      "Epoch: 1/2 Loss: 0.4499 Test accuracy: 0.8118\n",
      "Epoch: 1/2 Loss: 0.4722 Test accuracy: 0.8209\n",
      "Epoch: 1/2 Loss: 0.4455 Test accuracy: 0.7971\n",
      "Epoch: 1/2 Loss: 0.5024 Test accuracy: 0.8154\n",
      "Epoch: 1/2 Loss: 0.4923 Test accuracy: 0.8253\n",
      "Epoch: 1/2 Loss: 0.4376 Test accuracy: 0.8199\n",
      "Epoch: 1/2 Loss: 0.4604 Test accuracy: 0.8268\n",
      "Epoch: 1/2 Loss: 0.4377 Test accuracy: 0.8156\n",
      "Epoch: 1/2 Loss: 0.4868 Test accuracy: 0.8314\n",
      "Epoch: 1/2 Loss: 0.4036 Test accuracy: 0.8299\n",
      "Epoch: 1/2 Loss: 0.4656 Test accuracy: 0.8205\n",
      "Epoch: 1/2 Loss: 0.4536 Test accuracy: 0.8255\n",
      "Epoch: 1/2 Loss: 0.4327 Test accuracy: 0.8312\n",
      "Epoch: 1/2 Loss: 0.5017 Test accuracy: 0.8324\n",
      "Epoch: 1/2 Loss: 0.4470 Test accuracy: 0.8257\n",
      "Epoch: 1/2 Loss: 0.4483 Test accuracy: 0.8028\n",
      "Epoch: 1/2 Loss: 0.5205 Test accuracy: 0.8126\n",
      "Epoch: 1/2 Loss: 0.4672 Test accuracy: 0.8194\n",
      "Epoch: 1/2 Loss: 0.4127 Test accuracy: 0.8234\n",
      "Epoch: 1/2 Loss: 0.4827 Test accuracy: 0.8216\n",
      "Epoch: 1/2 Loss: 0.4483 Test accuracy: 0.8364\n",
      "Epoch: 1/2 Loss: 0.4330 Test accuracy: 0.8221\n",
      "Epoch: 1/2 Loss: 0.4576 Test accuracy: 0.8319\n",
      "Epoch: 1/2 Loss: 0.4373 Test accuracy: 0.8340\n",
      "Epoch: 1/2 Loss: 0.4001 Test accuracy: 0.8388\n",
      "Epoch: 1/2 Loss: 0.4141 Test accuracy: 0.8309\n",
      "Epoch: 1/2 Loss: 0.4715 Test accuracy: 0.8213\n",
      "Epoch: 1/2 Loss: 0.4604 Test accuracy: 0.8054\n",
      "Epoch: 1/2 Loss: 0.4834 Test accuracy: 0.8017\n",
      "Epoch: 1/2 Loss: 0.4685 Test accuracy: 0.8363\n",
      "Epoch: 1/2 Loss: 0.3950 Test accuracy: 0.8334\n",
      "Epoch: 1/2 Loss: 0.3989 Test accuracy: 0.8313\n",
      "Epoch: 1/2 Loss: 0.4413 Test accuracy: 0.8348\n",
      "Epoch: 1/2 Loss: 0.4635 Test accuracy: 0.8384\n",
      "Epoch: 1/2 Loss: 0.3662 Test accuracy: 0.8393\n",
      "Epoch: 1/2 Loss: 0.3784 Test accuracy: 0.8388\n",
      "Epoch: 1/2 Loss: 0.3910 Test accuracy: 0.8349\n",
      "Epoch: 1/2 Loss: 0.4293 Test accuracy: 0.8383\n",
      "Epoch: 1/2 Loss: 0.4419 Test accuracy: 0.8439\n",
      "Epoch: 1/2 Loss: 0.4045 Test accuracy: 0.8314\n",
      "Epoch: 1/2 Loss: 0.4448 Test accuracy: 0.8208\n",
      "Epoch: 1/2 Loss: 0.4765 Test accuracy: 0.8344\n",
      "Epoch: 1/2 Loss: 0.4321 Test accuracy: 0.8370\n",
      "Epoch: 1/2 Loss: 0.4642 Test accuracy: 0.8446\n",
      "Epoch: 1/2 Loss: 0.4438 Test accuracy: 0.8376\n",
      "Epoch: 1/2 Loss: 0.3959 Test accuracy: 0.8364\n",
      "Epoch: 1/2 Loss: 0.4414 Test accuracy: 0.8408\n",
      "Epoch: 1/2 Loss: 0.4065 Test accuracy: 0.8340\n",
      "Epoch: 1/2 Loss: 0.4268 Test accuracy: 0.8351\n",
      "Epoch: 1/2 Loss: 0.4502 Test accuracy: 0.8438\n",
      "Epoch: 1/2 Loss: 0.4090 Test accuracy: 0.8461\n",
      "Epoch: 1/2 Loss: 0.4793 Test accuracy: 0.8383\n",
      "Epoch: 1/2 Loss: 0.4386 Test accuracy: 0.8427\n",
      "Epoch: 1/2 Loss: 0.3859 Test accuracy: 0.8428\n",
      "Epoch: 1/2 Loss: 0.4950 Test accuracy: 0.8398\n",
      "Epoch: 1/2 Loss: 0.4161 Test accuracy: 0.8407\n",
      "Epoch: 1/2 Loss: 0.4431 Test accuracy: 0.8330\n",
      "Epoch: 1/2 Loss: 0.4090 Test accuracy: 0.8406\n",
      "Epoch: 1/2 Loss: 0.3487 Test accuracy: 0.8377\n",
      "Epoch: 1/2 Loss: 0.4282 Test accuracy: 0.8368\n",
      "Epoch: 2/2 Loss: 0.3679 Test accuracy: 0.8453\n",
      "Epoch: 2/2 Loss: 0.3853 Test accuracy: 0.8441\n",
      "Epoch: 2/2 Loss: 0.3909 Test accuracy: 0.8462\n",
      "Epoch: 2/2 Loss: 0.3229 Test accuracy: 0.8257\n",
      "Epoch: 2/2 Loss: 0.4127 Test accuracy: 0.8398\n",
      "Epoch: 2/2 Loss: 0.3569 Test accuracy: 0.8435\n",
      "Epoch: 2/2 Loss: 0.3597 Test accuracy: 0.8397\n",
      "Epoch: 2/2 Loss: 0.4444 Test accuracy: 0.8433\n",
      "Epoch: 2/2 Loss: 0.4304 Test accuracy: 0.8461\n",
      "Epoch: 2/2 Loss: 0.4189 Test accuracy: 0.8389\n",
      "Epoch: 2/2 Loss: 0.3904 Test accuracy: 0.8444\n",
      "Epoch: 2/2 Loss: 0.3991 Test accuracy: 0.8364\n",
      "Epoch: 2/2 Loss: 0.4236 Test accuracy: 0.8287\n",
      "Epoch: 2/2 Loss: 0.3532 Test accuracy: 0.8390\n",
      "Epoch: 2/2 Loss: 0.3708 Test accuracy: 0.8513\n",
      "Epoch: 2/2 Loss: 0.3540 Test accuracy: 0.8343\n",
      "Epoch: 2/2 Loss: 0.3808 Test accuracy: 0.8452\n",
      "Epoch: 2/2 Loss: 0.4176 Test accuracy: 0.8294\n",
      "Epoch: 2/2 Loss: 0.4236 Test accuracy: 0.8477\n",
      "Epoch: 2/2 Loss: 0.3692 Test accuracy: 0.8510\n",
      "Epoch: 2/2 Loss: 0.4221 Test accuracy: 0.8413\n",
      "Epoch: 2/2 Loss: 0.4148 Test accuracy: 0.8272\n",
      "Epoch: 2/2 Loss: 0.4209 Test accuracy: 0.8332\n",
      "Epoch: 2/2 Loss: 0.3761 Test accuracy: 0.8402\n",
      "Epoch: 2/2 Loss: 0.3843 Test accuracy: 0.8469\n",
      "Epoch: 2/2 Loss: 0.3444 Test accuracy: 0.8542\n",
      "Epoch: 2/2 Loss: 0.4398 Test accuracy: 0.8415\n",
      "Epoch: 2/2 Loss: 0.3853 Test accuracy: 0.8260\n",
      "Epoch: 2/2 Loss: 0.3893 Test accuracy: 0.8484\n",
      "Epoch: 2/2 Loss: 0.3723 Test accuracy: 0.8415\n",
      "Epoch: 2/2 Loss: 0.3940 Test accuracy: 0.8556\n",
      "Epoch: 2/2 Loss: 0.3701 Test accuracy: 0.8492\n",
      "Epoch: 2/2 Loss: 0.3706 Test accuracy: 0.8615\n",
      "Epoch: 2/2 Loss: 0.3962 Test accuracy: 0.8530\n",
      "Epoch: 2/2 Loss: 0.4011 Test accuracy: 0.8521\n",
      "Epoch: 2/2 Loss: 0.4309 Test accuracy: 0.8515\n",
      "Epoch: 2/2 Loss: 0.3744 Test accuracy: 0.8544\n",
      "Epoch: 2/2 Loss: 0.3729 Test accuracy: 0.8465\n",
      "Epoch: 2/2 Loss: 0.4108 Test accuracy: 0.8487\n",
      "Epoch: 2/2 Loss: 0.3899 Test accuracy: 0.8571\n",
      "Epoch: 2/2 Loss: 0.3590 Test accuracy: 0.8542\n",
      "Epoch: 2/2 Loss: 0.3689 Test accuracy: 0.8510\n",
      "Epoch: 2/2 Loss: 0.3773 Test accuracy: 0.8470\n",
      "Epoch: 2/2 Loss: 0.3865 Test accuracy: 0.8490\n",
      "Epoch: 2/2 Loss: 0.3787 Test accuracy: 0.8563\n",
      "Epoch: 2/2 Loss: 0.3581 Test accuracy: 0.8447\n",
      "Epoch: 2/2 Loss: 0.3629 Test accuracy: 0.8584\n",
      "Epoch: 2/2 Loss: 0.3972 Test accuracy: 0.8460\n",
      "Epoch: 2/2 Loss: 0.3563 Test accuracy: 0.8468\n",
      "Epoch: 2/2 Loss: 0.4174 Test accuracy: 0.8510\n",
      "Epoch: 2/2 Loss: 0.3434 Test accuracy: 0.8559\n",
      "Epoch: 2/2 Loss: 0.3230 Test accuracy: 0.8581\n",
      "Epoch: 2/2 Loss: 0.3467 Test accuracy: 0.8557\n",
      "Epoch: 2/2 Loss: 0.3224 Test accuracy: 0.8540\n",
      "Epoch: 2/2 Loss: 0.3175 Test accuracy: 0.8534\n",
      "Epoch: 2/2 Loss: 0.3814 Test accuracy: 0.8552\n",
      "Epoch: 2/2 Loss: 0.4092 Test accuracy: 0.8565\n",
      "Epoch: 2/2 Loss: 0.3782 Test accuracy: 0.8498\n",
      "Epoch: 2/2 Loss: 0.3688 Test accuracy: 0.8473\n",
      "Epoch: 2/2 Loss: 0.4154 Test accuracy: 0.8548\n",
      "Epoch: 2/2 Loss: 0.4197 Test accuracy: 0.8564\n",
      "Epoch: 2/2 Loss: 0.3677 Test accuracy: 0.8521\n",
      "Epoch: 2/2 Loss: 0.3254 Test accuracy: 0.8592\n",
      "Epoch: 2/2 Loss: 0.3245 Test accuracy: 0.8450\n",
      "Epoch: 2/2 Loss: 0.4112 Test accuracy: 0.8515\n",
      "Epoch: 2/2 Loss: 0.3661 Test accuracy: 0.8459\n",
      "Epoch: 2/2 Loss: 0.3534 Test accuracy: 0.8626\n",
      "Epoch: 2/2 Loss: 0.3744 Test accuracy: 0.8479\n",
      "Epoch: 2/2 Loss: 0.4243 Test accuracy: 0.8564\n",
      "Epoch: 2/2 Loss: 0.3897 Test accuracy: 0.8521\n",
      "Epoch: 2/2 Loss: 0.3081 Test accuracy: 0.8540\n",
      "Epoch: 2/2 Loss: 0.3965 Test accuracy: 0.8389\n",
      "Epoch: 2/2 Loss: 0.3303 Test accuracy: 0.8463\n",
      "Epoch: 2/2 Loss: 0.3006 Test accuracy: 0.8538\n",
      "Epoch: 2/2 Loss: 0.3560 Test accuracy: 0.8612\n",
      "Epoch: 2/2 Loss: 0.4343 Test accuracy: 0.8516\n",
      "Epoch: 2/2 Loss: 0.4571 Test accuracy: 0.8436\n",
      "Epoch: 2/2 Loss: 0.4274 Test accuracy: 0.8397\n",
      "Epoch: 2/2 Loss: 0.4040 Test accuracy: 0.8579\n",
      "Epoch: 2/2 Loss: 0.3380 Test accuracy: 0.8580\n",
      "Epoch: 2/2 Loss: 0.3258 Test accuracy: 0.8471\n",
      "Epoch: 2/2 Loss: 0.4067 Test accuracy: 0.8434\n",
      "Epoch: 2/2 Loss: 0.3423 Test accuracy: 0.8524\n",
      "Epoch: 2/2 Loss: 0.3488 Test accuracy: 0.8376\n",
      "Epoch: 2/2 Loss: 0.3672 Test accuracy: 0.8551\n",
      "Epoch: 2/2 Loss: 0.3533 Test accuracy: 0.8535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/2 Loss: 0.3585 Test accuracy: 0.8607\n",
      "Epoch: 2/2 Loss: 0.3335 Test accuracy: 0.8553\n",
      "Epoch: 2/2 Loss: 0.3139 Test accuracy: 0.8561\n",
      "Epoch: 2/2 Loss: 0.3802 Test accuracy: 0.8625\n",
      "Epoch: 2/2 Loss: 0.3692 Test accuracy: 0.8564\n",
      "Epoch: 2/2 Loss: 0.2829 Test accuracy: 0.8555\n",
      "Epoch: 2/2 Loss: 0.3923 Test accuracy: 0.8494\n",
      "Epoch: 2/2 Loss: 0.3896 Test accuracy: 0.8558\n"
     ]
    }
   ],
   "source": [
    "epochs = 2\n",
    "steps = 0\n",
    "running_loss = 0\n",
    "print_every = 10\n",
    "for e in range(epochs):\n",
    "    for images, labels in iter(trainloader):\n",
    "        steps += 1\n",
    "        # Flatten images into a 784 long vector\n",
    "        images.resize_(images.size()[0], 784)\n",
    "        \n",
    "        # Wrap images and labels in Variables so we can calculate gradients\n",
    "        inputs = Variable(images)\n",
    "        targets = Variable(labels)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = net.forward(inputs)\n",
    "        loss = criterion(output, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.data[0]\n",
    "        \n",
    "        if steps % print_every == 0:\n",
    "            # Test accuracy\n",
    "            accuracy = 0\n",
    "            for ii, (images, labels) in enumerate(testloader):\n",
    "                \n",
    "                images = images.resize_(images.size()[0], 784)\n",
    "                inputs = Variable(images, volatile=True)\n",
    "                \n",
    "                predicted = torch.exp(net.forward(inputs).data)\n",
    "                equality = (labels == predicted.max(1)[1])\n",
    "                accuracy += equality.type_as(torch.FloatTensor()).mean()\n",
    "            \n",
    "            print(\"Epoch: {}/{}\".format(e+1, epochs),\n",
    "                  \"Loss: {:.4f}\".format(running_loss/print_every),\n",
    "                  \"Test accuracy: {:.4f}\".format(accuracy/(ii+1)))\n",
    "            running_loss = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and loading networks\n",
    "\n",
    "As you can imagine, it's impractical to train a network every time you need to use it. Instead, we can save trained networks then load them later to train more or use them for predictions. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(image, ax=None, title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    image = image.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.5, 0.5, 0.5])\n",
    "    std = np.array([0.5, 0.5, 0.5])\n",
    "    image = std * image + mean\n",
    "    image = np.clip(image, 0, 1)\n",
    "    ax.imshow(image)\n",
    "    \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
