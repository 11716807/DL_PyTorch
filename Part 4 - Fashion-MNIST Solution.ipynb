{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Exercise\n",
    "\n",
    "Now it's your turn to build a neural network. You'll be using the [Fashion-MNIST dataset](https://github.com/zalandoresearch/fashion-mnist), a drop-in replacement for the MNIST dataset. MNIST is actually quite trivial with neural networks where you can easily achieve better than 97% accuracy. Fashion-MNIST is a set of 28x28 greyscale images of clothes. It's more complex than MNIST, so it's a better representation of the actual performance of your network.\n",
    "\n",
    "<img src='assets/fashion-mnist-sprite.png' width=500px>\n",
    "\n",
    "In this notebook, you'll build your own neural network. For the most part, you could just copy and paste the code from Part 3, but you wouldn't be learning. It's important for you to write the code yourself and get it to work. Feel free to consult the previous notebook though as you work through this.\n",
    "\n",
    "First off, let's load the dataset through torchvision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import helper\n",
    "\n",
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "# Download and load the training data\n",
    "trainset = datasets.FashionMNIST('F_MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Download and load the test data\n",
    "testset = datasets.FashionMNIST('F_MNIST_data/', download=True, train=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see one of the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10b28f198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image, label = next(iter(trainloader))\n",
    "helper.imshow(image[0,:]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the data loaded, it's time to import the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import helper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the network\n",
    "\n",
    "Here you should define your network. As with MNIST, each image is 28x28 which is a total of 784 pixels, and there are 10 classes. You should include at least one hidden layer. We suggest you use ReLU activations for the layers and softmax for the output to get the class probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Sequential):\n",
    "    def __init__(self, input_size, output_size, hidden_layers):\n",
    "        super().__init__()\n",
    "        # Input to a hidden layer\n",
    "        self.fc1 = nn.Linear(input_size, hidden_layers[0])\n",
    "        \n",
    "        # Variable number of more hidden layers\n",
    "        self.hidden_layers = [nn.Linear(h1, h2) for h1, h2 in zip(hidden_layers[:-1], hidden_layers[1:])]\n",
    "        \n",
    "        self.output = nn.Linear(hidden_layers[-1], output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ''' Forward pass through the network, returns the output logits '''\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        \n",
    "        for each in self.hidden_layers:\n",
    "            x = F.relu(each(x))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the network\n",
    "\n",
    "Now you should create your network and train it. First you'll want to define [the criterion](http://pytorch.org/docs/master/nn.html#loss-functions) ( something like `nn.CrossEntropyLoss`) and [the optimizer](http://pytorch.org/docs/master/optim.html) (typically `optim.SGD` or `optim.Adam`).\n",
    "\n",
    "Then write the training code. Remember the training pass is a fairly straightforward process:\n",
    "\n",
    "* Make a forward pass through the network to get the logits \n",
    "* Use the logits to calculate the loss\n",
    "* Perform a backward pass through the network with `loss.backward()` to calculate the gradients\n",
    "* Take a step with the optimizer to update the weights\n",
    "\n",
    "By adjusting the hyperparameters (hidden units, learning rate, etc), you should be able to get 85-86% validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (fc1): Linear(in_features=784, out_features=200)\n",
       "  (output): Linear(in_features=200, out_features=10)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the network, define the criterion and optimizer\n",
    "net = Network(784, 10, [200])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'optimizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-0a2f5796bf0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'optimizer' is not defined"
     ]
    }
   ],
   "source": [
    "epochs = 2\n",
    "steps = 0\n",
    "running_loss = 0\n",
    "print_every = 10\n",
    "for e in range(epochs):\n",
    "    for images, labels in iter(trainloader):\n",
    "        steps += 1\n",
    "        # Flatten images into a 784 long vector\n",
    "        images.resize_(images.size()[0], 784)\n",
    "        \n",
    "        # Wrap images and labels in Variables so we can calculate gradients\n",
    "        inputs = Variable(images)\n",
    "        targets = Variable(labels)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = net.forward(inputs)\n",
    "        loss = criterion(output, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.data[0]\n",
    "        \n",
    "        if steps % print_every == 0:\n",
    "            # Test accuracy\n",
    "            accuracy = 0\n",
    "            for ii, (images, labels) in enumerate(testloader):\n",
    "                \n",
    "                images = images.resize_(images.size()[0], 784)\n",
    "                inputs = Variable(images, volatile=True)\n",
    "                \n",
    "                predicted = torch.exp(net.forward(inputs).data)\n",
    "                equality = (labels == predicted.max(1)[1])\n",
    "                accuracy += equality.type_as(torch.FloatTensor()).mean()\n",
    "            \n",
    "            print(\"Epoch: {}/{}\".format(e+1, epochs),\n",
    "                  \"Loss: {:.4f}\".format(running_loss/print_every),\n",
    "                  \"Test accuracy: {:.4f}\".format(accuracy/(ii+1)))\n",
    "            running_loss = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and loading networks\n",
    "\n",
    "As you can imagine, it's impractical to train a network every time you need to use it. Instead, we can save trained networks then load them later to train more or use them for predictions. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(image, ax=None, title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    image = image.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.5, 0.5, 0.5])\n",
    "    std = np.array([0.5, 0.5, 0.5])\n",
    "    image = std * image + mean\n",
    "    image = np.clip(image, 0, 1)\n",
    "    ax.imshow(image)\n",
    "    \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
